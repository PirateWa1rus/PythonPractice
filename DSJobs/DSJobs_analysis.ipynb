{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cda282",
   "metadata": {},
   "source": [
    "<center><h1><strong>Data Science EDA</strong></h1><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74946968",
   "metadata": {},
   "source": [
    "This notebook contains exploratory data analysis of Data Science Roles, Skills & Salaries 2025 dataset from Kaggle (https://www.kaggle.com/datasets/sidraaazam/data-science-roles-skills-and-salaries-2025). The purpose of this analysis is for me to practice some basic EDA and Python skills with a dataset that will hopefully provide insights that inform my own self-education and job search in the field of data analytics and data science.\n",
    "\n",
    "Here are the main questions guiding my analysis:\n",
    "1. What skills are highly sought after among Data Science roles and which skills are most desired (as reflected in salaries)?\n",
    "2. How are jobs and skills distributed across industries?\n",
    "3. Do insights gained through this EDA provide guidance for my own skill and portfolio development?\n",
    "\n",
    "As is my personal preference, the code contained herein is heavily editorialized with comments. The notebook will also conclude with a summary of skills, tools, and resources used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd11484",
   "metadata": {},
   "source": [
    "<center><h2><strong>Data</strong></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46637889",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing modules that are probably just generically useful for most data analysis scripts.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## Now it's time to get that data...\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"data_science_job_posts_2025 (2).csv\"\n",
    "\n",
    "df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS,\"sidraaazam/data-science-roles-skills-and-salaries-2025\",file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26424c52",
   "metadata": {},
   "source": [
    "<center><h2><strong>Cleaning</strong></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'll start with salaries. After taking a look at the dataset, I've realized that salaries are all presented in '€'(thankfully).\n",
    "## Some are presented as single values, while others are ranges. I'd like to strip out the '€' symbols, commas, and convert ranges to averages.\n",
    "## I'll define a function that will clean this up for me.\n",
    "\n",
    "def salary_clean(salary):\n",
    "    if pd.isna(salary):\n",
    "        return np.nan\n",
    "    salary = salary.replace('€', '').replace(',', '').strip()\n",
    "    if '-' in salary:\n",
    "        low, high = salary.split('-')\n",
    "        return (float(low) + float(high)) / 2\n",
    "    else:\n",
    "        try:\n",
    "            return float(salary)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "## Now I'll apply this function to the 'Salary' column in the dataframe, since I'll need this cleaned version from the start.\n",
    "df['Cleaned_Salary'] = df['salary'].apply(salary_clean).astype(float)\n",
    "\n",
    "## Similarly, I'll need a function to clean up revenues. I won't need this until later\n",
    "\n",
    "def rev_clean(rev):\n",
    "         if pd.isna(rev):\n",
    "             return np.nan\n",
    "         if rev in ['Public', 'Private', 'Education', 'Nonprofit']:\n",
    "             return np.nan\n",
    "         rev = rev.replace('€', '').replace(',', '').strip()\n",
    "         if rev.endswith('B'):\n",
    "             return float(rev[:-1])\n",
    "         if rev.endswith('T'):\n",
    "             return float(rev[:-1])*1000\n",
    "         if rev.endswith('M'):\n",
    "             return float(rev[:-1])/1000\n",
    "\n",
    "## This will remove the € symbol and the trailing M, T, and B values while converting all values to floats (in billions of Euros).\n",
    "## Now we'll apply this to the revenues field.\n",
    "         \n",
    "## Early analysis revealed that there are three strongly outlying salaries in the dataset.\n",
    "## In order to avoid repeated skewing of results, I mostly worked with a filtered version of the dataset.\n",
    "df_filtered = df[df['Cleaned_Salary'] < 600000].copy()\n",
    "\n",
    "## When looking at how seniority level affects salary, I'll lump junior + midlevel and senior + lead roles.\n",
    "df_filtered.loc[:, 'Seniority_Binary'] = df_filtered['seniority_level'].apply(lambda x: 'junior' if x in ['junior', 'midlevel'] else 'senior' if pd.notna(x) else np.nan)\n",
    "\n",
    "## During the skills analysis, I'll need lists of the individual skills for Junior roles...\n",
    "jr_subset = df_filtered[df_filtered['Seniority_Binary'] == 'junior']['skills'].dropna()\n",
    "jr_skill_counter = Counter()\n",
    "for skills in jr_subset:\n",
    "    skill_list = [skill.strip(\" []'\\\"\").lower() for skill in skills.split(',') if skill.strip(\" []'\\\"\")]\n",
    "    jr_skill_counter.update(skill_list)\n",
    "most_common_jr_skills = jr_skill_counter.most_common(25)\n",
    "\n",
    "## ... and for Senior roles.\n",
    "sr_subset = df_filtered[df_filtered['Seniority_Binary'] == 'senior']['skills'].dropna()\n",
    "sr_skill_counter = Counter()\n",
    "for skills in sr_subset:\n",
    "    skill_list = [skill.strip(\" []'\\\"\").lower() for skill in skills.split(',') if skill.strip(\" []'\\\"\")]\n",
    "    sr_skill_counter.update(skill_list)\n",
    "most_common_sr_skills = sr_skill_counter.most_common(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75a2a4",
   "metadata": {},
   "source": [
    "<center><h2><strong>Exploratory Data Analysis</strong></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb75ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that I have cleaned up salary info, I think maybe we'll make a histogram of the salaries.\n",
    "df_filtered['Cleaned_Salary'].plot.hist(bins=30, edgecolor='black')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Data Scientist Salaries')\n",
    "plt.grid(True)\n",
    "plt.show(block = False)  \n",
    "\n",
    "## I'd like to see what the distribution of salaries is like when broken down by experience level.\n",
    "## I'll make a boxplot for this. \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='seniority_level', y='Cleaned_Salary', data=df_filtered, showmeans=True, meanline=True)\n",
    "plt.xlabel('Experience Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.title('Salary Distribution by Experience Level')\n",
    "plt.grid(True)\n",
    "plt.show(block = False)\n",
    "\n",
    "## The boxplot suggests that \"junior\" and \"midlevel\" might not really be meaningful distinctions.\n",
    "## For now, I think I'd like to make histograms of salaries by experience level all shown together in one window as separate plots.\n",
    "\n",
    "experience_levels = df_filtered['seniority_level'].dropna().unique()\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 5), sharey=False)\n",
    "axes = axes.flatten()\n",
    "for ax, level in zip(axes, experience_levels):\n",
    "    sns.histplot(data=df_filtered[df_filtered['seniority_level'] == level], x='Cleaned_Salary', bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(level)\n",
    "    ax.set_xlabel('Salary')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.suptitle('Salary Distribution by Experience Level')\n",
    "plt.tight_layout()\n",
    "plt.show(block = False)\n",
    "\n",
    "## These plots make it clearer that \"junior\" has far fewer records than the other categories, and distributions are not exactly normal.\n",
    "## I'll try recategorizing seniority_level into just \"junior\" and \"senior\" (combining midlevel and senior) and see how that looks.\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), sharey=False)\n",
    "axes = axes.flatten()\n",
    "for ax, level in zip(axes, ['junior', 'senior']):\n",
    "    sns.histplot(data=df_filtered[df_filtered['Seniority_Binary'] == level], x='Cleaned_Salary', bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(level)\n",
    "    ax.set_xlabel('Salary')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.suptitle('Salary Distribution by Lumped Experience Level')\n",
    "plt.tight_layout()\n",
    "plt.show(block = False)\n",
    "\n",
    "input('Press Enter to continue...')\n",
    "\n",
    "## These are actually pretty interesting results. We can pretty clearly see that the \"junior\" category is not normally distributed.\n",
    "## The \"Senior\" category is closer to normal, but still has a bit of a right skew.\n",
    "## I'll be curious to see if particular skills are associated with identifiable distribution patterns.\n",
    "\n",
    "## To get started, I'll need to learn something about what skills are listed.\n",
    "## I took a look at the skills data, and I'm seeing that most entries are comma-separated lists of skills.\n",
    "## What I'd like to do is understand which individual skills appear most often for 'junior' roles and 'senior' roles.\n",
    "## I'll compare the top skills for both levels.\n",
    "print('\\nTop Skills for Junior and Senior DS Roles')\n",
    "print(f\"{'No.':<5} {'Junior Skill':<20} {'Count':<7} {'Senior Skill':<20} {'Count':<7}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(max(len(most_common_jr_skills), len(most_common_sr_skills))):\n",
    "    jr_skill, jr_count = most_common_jr_skills[i]\n",
    "    sr_skill, sr_count = most_common_sr_skills[i]\n",
    "    print(f\"{i+1:<5} {jr_skill:<20} {jr_count:<7} {sr_skill:<20} {sr_count:<7}\")\n",
    "\n",
    "## For the most part, these lists are in agreement. I'm honestly a little surprised not to see more differentiation.\n",
    "## I think what would be interesting is to see which skills are associated with higher salaries within each experience level.\n",
    "## Let's start with junior roles.\n",
    "jr_skill_salary = {}\n",
    "for skills, salary in zip(df_filtered[df_filtered['Seniority_Binary'] == 'junior']['skills'].dropna(), df_filtered[df_filtered['Seniority_Binary'] == 'junior']['Cleaned_Salary'].dropna()):\n",
    "    skill_list = [skill.strip(\" []'\\\"\").lower() for skill in skills.split(',') if skill.strip(\" []'\\\"\")]\n",
    "    for skill in skill_list:\n",
    "        if skill not in jr_skill_salary:\n",
    "            jr_skill_salary[skill] = []\n",
    "        jr_skill_salary[skill].append(salary)\n",
    "\n",
    "## What that did was create a dictionary where each skill maps to a list of salaries for junior roles requiring that skill.\n",
    "## Now let's get some summary stats for each skill.\n",
    "jr_skill_salary_summary = {skill: (np.mean(salaries), np.median(salaries), np.std(salaries), len(salaries)) for skill, salaries in jr_skill_salary.items() if len(salaries) >= 2}\n",
    "jr_skill_salary_summary_sorted = sorted(jr_skill_salary_summary.items(), key=lambda x: x[1][0], reverse=True)\n",
    "## Now we have a dictionary containing skills and summary stats, sorted by mean salary.\n",
    "## Let's print out a table with the top 25 skills and their associated salary stats.\n",
    "print('\\nJunior Level DS Skills and Salary Stats')\n",
    "print(f\"{'No.':<5} {'Skill':<20} {'Mean Salary':<15} {'Median Salary':<15} {'Std Dev':<10} {'Count':<7}\")\n",
    "print(\"-\" * 80)\n",
    "for i, (skill, (mean_salary, median_salary, std_dev, count)) in enumerate(jr_skill_salary_summary_sorted[:25]):\n",
    "    print(f\"{i+1:<5} {skill:<20} {mean_salary:<15.2f} {median_salary:<15.2f} {std_dev:<10.2f} {count:<7}\")\n",
    "    \n",
    "## One thing I'm noticing is that these standard deviations are very high.\n",
    "## What this tells me is that the roles I've broadly classified as \"junior\" probably represent a wide range of actual experience levels.\n",
    "## My theory at this point would be that the top skills are really just the \"tools of the trade\" and that, for these junior roles at least,\n",
    "## there's unlikely to be a strong correlation between specific skills and salary. Also, pay for some of these jobs is shockingly low.\n",
    "## At some point later I might want to see if the covariance of skill pairs for some of these tops skills is better correlated with salary.\n",
    "## For now, I'll take a look at senior roles next and see if there's more differentiation there.\n",
    "sr_skill_salary = {}\n",
    "for skills, salary in zip(df_filtered[df_filtered['Seniority_Binary'] == 'senior']['skills'].dropna(), df_filtered[df_filtered['Seniority_Binary'] == 'senior']['Cleaned_Salary'].dropna()):\n",
    "    skill_list = [skill.strip(\" []'\\\"\").lower() for skill in skills.split(',') if skill.strip(\" []'\\\"\")]\n",
    "    for skill in skill_list:\n",
    "        if skill not in sr_skill_salary:\n",
    "            sr_skill_salary[skill] = []\n",
    "        sr_skill_salary[skill].append(salary)\n",
    "\n",
    "\n",
    "sr_skill_salary_summary = {skill: (np.mean(salaries), np.median(salaries), np.std(salaries), len(salaries)) for skill, salaries in sr_skill_salary.items() if len(salaries) >= 2}\n",
    "sr_skill_salary_summary_sorted = sorted(sr_skill_salary_summary.items(), key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "print('\\nSenior Level DS Skills and Salary Stats')\n",
    "print(f\"{'No.':<5} {'Skill':<20} {'Mean Salary':<15} {'Median Salary':<15} {'Std Dev':<10} {'Count':<7}\")\n",
    "print(\"-\" * 80)\n",
    "for i, (skill, (mean_salary, median_salary, std_dev, count)) in enumerate(sr_skill_salary_summary_sorted[:25]):\n",
    "    print(f\"{i+1:<5} {skill:<20} {mean_salary:<15.2f} {median_salary:<15.2f} {std_dev:<10.2f} {count:<7}\")\n",
    "\n",
    "## Ok, now we're getting somwhere. Scala is obviously showing itself as a high-value skill. My bet is that this has to do with who is hiring.\n",
    "## That is to say, it's probably big companies who are utilizing Scala for large dataset processing, automations, etc.\n",
    "## I'll have to do more reasearch on that (and see if I can find a way to work with Scala for a project in my portfolio),\n",
    "## but the language itself doesn't really look to be that different from Python or R for data science tasks. If a company has already sunk\n",
    "## significant resources into Scala, though, that would explain why they are willing to pay a premium for it.\n",
    "## Same is likely the case for Spark and some the other big data tools we're seeing here.\n",
    "## Interestingly, bash shows up in the senior skills, but not the junior skills. I would guess this is because senior roles are expected to own\n",
    "## more of the end-to-end data pipeline, which often involves some bash scripting for OS-level automations.\n",
    "## I'm also somewhat surprised not to see more generic skills (communication, teamwork, agile, MS Office, etc.).\n",
    "## Maybe these things aren't as necessary for these roles, as they might not be client-facing? or maybe they just assume everyone has them?\n",
    "\n",
    "\n",
    "## Let's look at job titles next.\n",
    "title_list = df['job_title'].dropna().unique()\n",
    "title_counts = df['job_title'].dropna().value_counts()\n",
    "\n",
    "for title in title_list:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        df_filtered[df_filtered['job_title']==title]['Cleaned_Salary'].plot.hist(bins=10, edgecolor = 'black')\n",
    "        plt.xlabel('Salary')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f\"Distribution of {title} Salaries\")\n",
    "        plt.grid(True)\n",
    "        plt.show(block = False)\n",
    "\n",
    "input('Press Enter to continue...')\n",
    "\n",
    "## Ok, we have an idea of the range of salaries and how they are distributed. We have an idea of the skills most commonly expected.\n",
    "## I think maybe the next step should be to see how salaries and skills vary by industry.\n",
    "\n",
    "industries = df_filtered['industry'].dropna().unique()\n",
    "industry_counts = df_filtered['industry'].dropna().value_counts()\n",
    "\n",
    "## This time, I'll make a pie chart of industries represented in the dataset, combining industries with fewer than 20 listings into an \"Other.\"\n",
    "## Then, I'll add a bar of the \"other\" slice so we can see how the smaller industries contibute without making the pie chart unreadable.\n",
    "\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "big_ind = industry_counts[industry_counts >= 20]\n",
    "lil_ind = industry_counts[industry_counts < 20]\n",
    "big_ind['Other'] = lil_ind.sum()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "## pie chart params\n",
    "pie_labels = big_ind.index\n",
    "pie_sizes = big_ind.values\n",
    "colors = plt.cm.tab20.colors[:len(pie_labels)]\n",
    "explode = [0.1 if size == big_ind['Other'] else 0 for size in pie_sizes]\n",
    "angle = 180*(pie_sizes.min()/sum(pie_sizes))\n",
    "## took me a while to understand this next part, but this will capture the slices from the .pie() call and junk the other outputs into var '_'.\n",
    "wedges, *_ = ax1.pie(pie_sizes, labels=pie_labels, autopct='%1.1f%%', startangle=angle, colors=colors, explode=explode)\n",
    "\n",
    "## bar chart params\n",
    "bar_labels = lil_ind.index\n",
    "bar_sizes = [v / sum(lil_ind.values) for v in lil_ind.values]\n",
    "bottom = 1\n",
    "width = 0.2\n",
    "\n",
    "## stack bars to match legend order\n",
    "for s, (height, label) in enumerate(reversed([*zip(bar_sizes, bar_labels)])):\n",
    "    bottom -= height\n",
    "    bc = ax2.bar(0, height, width, bottom=bottom, label=label, color='C0', alpha = 0.1 + 0.25 * s)\n",
    "    ## create bars\n",
    "    ax2.bar_label(bc, labels = [f\"{height:.0%}\"], label_type = 'center')\n",
    "\n",
    "## bar chart info\n",
    "ax2.set_title('Industries with Fewer than 20 Listings')\n",
    "ax2.legend(loc = 'best', bbox_to_anchor=(1, 0.965))\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(-2.5*width, 2.5*width)\n",
    "\n",
    "## Now we draw some lines connecting the pie to the bar chart.\n",
    "## Not sure that this will be drawing to the correct spots, but we'll see.\n",
    "theta1, theta2 = wedges[4].theta1, wedges[4].theta2\n",
    "center, r = wedges[4].center, wedges[4].r\n",
    "bar_height = sum(bar_sizes)\n",
    "\n",
    "## draw top connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta2) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta2) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, bar_height), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "con.set_linewidth(4)\n",
    "ax2.add_artist(con)\n",
    "\n",
    "## draw bottom connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta1) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta1) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, 0), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "ax2.add_artist(con)\n",
    "con.set_linewidth(4)\n",
    "\n",
    "plt.show(block = False)\n",
    "input('Press Enter to exit...')\n",
    "\n",
    "## That ended up being a pretty good looking plot. We're seeing that tech dominates the field, followed by finance, retail, and healthcare.\n",
    "## What could explain this distribution? Tech would obviously be the industry most burdened with large datasets, so that makes sense.\n",
    "## Finance would also be a data-heavy industry, with the added pressure to derive valueable insights from data either for driving trade decisions,\n",
    "## risk management, fraud detection, or customer insights. Retail would also have a lot of data, especially e-commerce, and an interest in converting\n",
    "## that data into customer insights. Healthcare is interesting, becuase I would assume that it's representation here is more about the similar pressures\n",
    "## on the industry to the others listed (lots of data, lots of customers, high cost of operation drives an emphasis on cost reduction and efficiency),\n",
    "## rather than a belief among healthcare companies that data science has a major role to play in directing care or R&D.\n",
    "\n",
    "## Next, I'll take a look at which skills are most commonly requested by industry.\n",
    "industry_skill_counter = {industry: Counter() for industry in industries}\n",
    "for industry in industries:\n",
    "    for skills in df_filtered[df_filtered['industry'] == industry]['skills'].dropna():\n",
    "        skill_list = [skill.strip(\" []'\\\"\").lower() for skill in skills.split(',') if skill.strip(\" []'\\\"\")]\n",
    "        industry_skill_counter[industry].update(skill_list)\n",
    "most_common_industry_skills = {industry: counter.most_common(25) for industry, counter in industry_skill_counter.items()}\n",
    "for industry, skills in most_common_industry_skills.items():\n",
    "    print(f\"\\nMost common skills for {industry} industry:\")\n",
    "    for skill, count in skills:\n",
    "        print(f\"{skill}: {count}\")\n",
    "\n",
    "## So, the most interesting thing I'm noticing here is Scala only appears as a top-ten skill in retail.\n",
    "## My suspicion is that it would show up more if we were looking at more than ten skills per industry. Let me take a look at that now...\n",
    "## Yup, when we look at the top 25 skills, Scala shows up in healthcare, tech, and energy as well.\n",
    "\n",
    "## Another intersting data point here is where we see deep learning as a major skill. This represents a real differentiation of the types\n",
    "## of data and insights sought by each industry. Retail, finance, tech, and healthcare all make sense here, as we can pretty easily intuit\n",
    "## the types of unstructured data (images, text, video, audio) that these industries would be handling, and how they might benefit from deep learning.\n",
    "\n",
    "## Although not as directly insightful at the scale of industry hiring trends, we can understand a difference in the types of roles being hired for\n",
    "## where we see skills like tableau and power bi. These are more business-intelligence focused skills, suggesting roles that are more focused\n",
    "## on presenting data insights to business stakeholders, rather than or in addition to building data products or automations.\n",
    "\n",
    "## Appart from that, we're mostly seeing the same skills and the same rankings for each industry.\n",
    "## The big takeaway here is probably that if you want to get a job as a data scientist you should know python, SQL, and machine learning.\n",
    "## Beyond that some familiarty with big data tools and infrastructure (Hadoop, Spark, AWS, etc.) is probably a good idea.\n",
    "## Depending on the role, some knowledge of deep learning and/or business intelligence tools might be useful as well.\n",
    "\n",
    "## Now I want to look at the association of company revenue and salary.\n",
    "## Like we did for salaries before, we'll need to clean up the revenues field using a custom function.\n",
    "\n",
    "def rev_clean(rev):\n",
    "         if pd.isna(rev):\n",
    "             return np.nan\n",
    "         if rev in ['Public', 'Private', 'Education', 'Nonprofit']:\n",
    "             return np.nan\n",
    "         rev = rev.replace('€', '').replace(',', '').strip()\n",
    "         if rev.endswith('B'):\n",
    "             return float(rev[:-1])\n",
    "         if rev.endswith('T'):\n",
    "             return float(rev[:-1])*1000\n",
    "         if rev.endswith('M'):\n",
    "             return float(rev[:-1])/1000\n",
    "\n",
    "## This will remove the € symbol and the trailing M, T, and B values while converting all values to floats (in billions of Euros).\n",
    "## Now we'll apply this to the revenues field.\n",
    "df_filtered.loc[:,'Cleaned_Revenue'] = df_filtered['revenue'].apply(rev_clean).astype(float)\n",
    "## Next, where both Cleaned_Salary and Cleaned_Revenue are not null, I'll create a scatterplot of the relationship between salary and revenue.\n",
    "## To do that, I need a version of the dataframe that only has records where both values are not null\n",
    "df_revclean = df_filtered.dropna(subset=['Cleaned_Revenue', 'Cleaned_Salary']).copy()\n",
    "## The cleaned revenues need to be in a 2D array for the .fit() and .score() functions\n",
    "X = df_revclean['Cleaned_Revenue'].values.reshape(-1,1)\n",
    "Y = df_revclean['Cleaned_Salary'].values\n",
    "## Next I'll create the scatter plot and plot a trend line. \n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "## Get an R-squared value by predicting values based on trend line and compare to actual salaries.\n",
    "## Like with X above, rev_range needs to be a 2D array for .predict()\n",
    "rev_range = np.linspace(X.min(), X.max(), 100).reshape(-1,1)\n",
    "pred_sal = model.predict(rev_range)\n",
    "r_squared = model.score(X, Y)\n",
    "## creat the scatter plot and plot the trend line labeled with R-squared value\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "ax.scatter(df_revclean['Cleaned_Revenue'], df_revclean['Cleaned_Salary'], color='teal', edgecolor='black', s=100)\n",
    "ax.plot(rev_range, pred_sal, color='darkorange', linewidth=2, label=f'Trend line (R-squared = {r_squared:.2f})')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.title('Relationship of DS Salary to Company Revenue')\n",
    "plt.xlabel('Company Revenue (Billions of Euros)')\n",
    "plt.ylabel('Salary (Euros)')\n",
    "plt.tight_layout()\n",
    "plt.show(block=False)\n",
    "input('Press enter to exit...')\n",
    "\n",
    "## From this plot we see that company revenue is not at all a good predictor of DS role salaries. The smallest companies in terms of revenue\n",
    "## have some of the highest salaries in the dataset, and while the trend line suggests that bigger companies hire at slightly higher salaries,\n",
    "## the R-squared value tells us that this association is extremely weak in our dataset. This shows a couple of interesting things when taken\n",
    "## together with our previous insights: First, there are probably a good number of tech start-ups who are willing to pay a real premium for a \n",
    "## skilled data science professional. Second, just because a company makes a lot of money, it does not mean that an applicant can expect a much\n",
    "## higher salary than they would receive from a smaller one. There is a clear pattern of higher variability among salaries at smaller companies,\n",
    "## so that may be an important point of consideration for the applicant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa06391",
   "metadata": {},
   "source": [
    "<center><h2><strong>Advanced Analysis</strong></h2><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TBC, this section will contain analysis of skills and salaries for Junior and Senior DS roles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea19e5f1",
   "metadata": {},
   "source": [
    "<center><h2><strong>Conclusions</strong></h2></center>\n",
    "\n",
    "This cell will contain takeaways. It might even make sense to have all of the editorial comments from the EDA section moved into here..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
